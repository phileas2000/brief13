{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>From: gld@cunixb.cc.columbia.edu (Gary L Dare)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>From: atterlep@vela.acs.oakland.edu (Cardinal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>From: miner@kuhub.cc.ukans.edu\\nSubject: Re: A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>From: atterlep@vela.acs.oakland.edu (Cardinal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>From: vzhivov@superior.carleton.ca (Vladimir Z...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  From: gld@cunixb.cc.columbia.edu (Gary L Dare)...\n",
       "1  From: atterlep@vela.acs.oakland.edu (Cardinal ...\n",
       "2  From: miner@kuhub.cc.ukans.edu\\nSubject: Re: A...\n",
       "3  From: atterlep@vela.acs.oakland.edu (Cardinal ...\n",
       "4  From: vzhivov@superior.carleton.ca (Vladimir Z..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data', sep=\",\", header=None)\n",
    "\n",
    "data.columns = ['text']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a collection of emails that are not labelled. Let's try extract topics from them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‡ You're used to it by now... Clean up! Store the cleaned text in a new dataframe column \"clean_text\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from  string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#df[\"text\"] = df['text'].str.lower()\n",
    "#df[\"text\"] = df['text'].str.replace('[0-9]','')\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "#df['text'] = df['text'].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‡ Train an LDA model to extract potential topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\philg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "data_words = list(sent_to_words(data['text']))# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "\n",
    "\n",
    "\n",
    "import gensim.corpora as corpora# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)# Create Corpus\n",
    "texts = data_words# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]# View\n",
    "\n",
    "\n",
    "from pprint import pprint# number of topics\n",
    "num_topics = 10# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word)# Print the Keyword in the 10 topics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize potential topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‡ The function to print the words associated with the potential topics is already made for you. You just have to pass the correct arguments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(79,\n",
      "  '0.009*\"edu\" + 0.007*\"hockey\" + 0.005*\"lines\" + 0.005*\"period\" + '\n",
      "  '0.004*\"play\" + 0.004*\"god\" + 0.004*\"nhl\" + 0.004*\"article\" + '\n",
      "  '0.004*\"organization\" + 0.004*\"one\"'),\n",
      " (74,\n",
      "  '0.007*\"edu\" + 0.007*\"would\" + 0.006*\"ca\" + 0.006*\"one\" + 0.005*\"subject\" + '\n",
      "  '0.005*\"lines\" + 0.005*\"organization\" + 0.005*\"god\" + 0.005*\"also\" + '\n",
      "  '0.004*\"writes\"'),\n",
      " (87,\n",
      "  '0.009*\"god\" + 0.006*\"ca\" + 0.006*\"subject\" + 0.006*\"second\" + 0.006*\"one\" + '\n",
      "  '0.005*\"coming\" + 0.005*\"organization\" + 0.005*\"lines\" + 0.005*\"edu\" + '\n",
      "  '0.004*\"christ\"'),\n",
      " (97,\n",
      "  '0.009*\"god\" + 0.007*\"edu\" + 0.006*\"lines\" + 0.005*\"subject\" + 0.005*\"one\" + '\n",
      "  '0.005*\"think\" + 0.004*\"know\" + 0.004*\"organization\" + 0.004*\"would\" + '\n",
      "  '0.004*\"ca\"'),\n",
      " (99,\n",
      "  '0.011*\"edu\" + 0.011*\"jesus\" + 0.009*\"subject\" + 0.008*\"would\" + '\n",
      "  '0.007*\"lines\" + 0.006*\"god\" + 0.006*\"one\" + 0.005*\"organization\" + '\n",
      "  '0.005*\"know\" + 0.005*\"go\"'),\n",
      " (28,\n",
      "  '0.009*\"god\" + 0.005*\"subject\" + 0.005*\"church\" + 0.005*\"one\" + '\n",
      "  '0.005*\"jesus\" + 0.005*\"people\" + 0.005*\"period\" + 0.004*\"organization\" + '\n",
      "  '0.004*\"would\" + 0.004*\"edu\"'),\n",
      " (52,\n",
      "  '0.009*\"would\" + 0.008*\"edu\" + 0.006*\"subject\" + 0.005*\"game\" + '\n",
      "  '0.005*\"lines\" + 0.004*\"play\" + 0.004*\"period\" + 0.004*\"organization\" + '\n",
      "  '0.004*\"god\" + 0.004*\"one\"'),\n",
      " (49,\n",
      "  '0.009*\"edu\" + 0.007*\"one\" + 0.007*\"god\" + 0.006*\"lines\" + 0.005*\"subject\" + '\n",
      "  '0.005*\"organization\" + 0.005*\"writes\" + 0.004*\"play\" + 0.004*\"ca\" + '\n",
      "  '0.004*\"com\"'),\n",
      " (54,\n",
      "  '0.006*\"organization\" + 0.006*\"edu\" + 0.006*\"hockey\" + 0.006*\"know\" + '\n",
      "  '0.005*\"people\" + 0.005*\"lines\" + 0.005*\"subject\" + 0.005*\"would\" + '\n",
      "  '0.004*\"may\" + 0.004*\"one\"'),\n",
      " (32,\n",
      "  '0.007*\"edu\" + 0.007*\"god\" + 0.007*\"one\" + 0.006*\"jesus\" + 0.006*\"lines\" + '\n",
      "  '0.005*\"subject\" + 0.005*\"would\" + 0.005*\"organization\" + 0.004*\"think\" + '\n",
      "  '0.004*\"people\"'),\n",
      " (71,\n",
      "  '0.013*\"edu\" + 0.009*\"gm\" + 0.006*\"hockey\" + 0.006*\"subject\" + 0.005*\"like\" '\n",
      "  '+ 0.005*\"lines\" + 0.005*\"would\" + 0.005*\"one\" + 0.005*\"team\" + '\n",
      "  '0.005*\"organization\"'),\n",
      " (33,\n",
      "  '0.010*\"god\" + 0.008*\"would\" + 0.006*\"edu\" + 0.005*\"subject\" + 0.005*\"one\" + '\n",
      "  '0.004*\"ca\" + 0.004*\"lines\" + 0.004*\"two\" + 0.004*\"organization\" + '\n",
      "  '0.003*\"writes\"'),\n",
      " (88,\n",
      "  '0.008*\"edu\" + 0.006*\"people\" + 0.005*\"subject\" + 0.005*\"god\" + 0.005*\"tor\" '\n",
      "  '+ 0.005*\"buf\" + 0.004*\"det\" + 0.004*\"university\" + 0.004*\"nyr\" + '\n",
      "  '0.004*\"lines\"'),\n",
      " (65,\n",
      "  '0.008*\"edu\" + 0.007*\"would\" + 0.007*\"lines\" + 0.007*\"subject\" + 0.006*\"one\" '\n",
      "  '+ 0.005*\"organization\" + 0.005*\"article\" + 0.005*\"writes\" + 0.005*\"think\" + '\n",
      "  '0.005*\"com\"'),\n",
      " (29,\n",
      "  '0.011*\"edu\" + 0.008*\"god\" + 0.006*\"subject\" + 0.006*\"organization\" + '\n",
      "  '0.005*\"lines\" + 0.005*\"would\" + 0.004*\"time\" + 0.004*\"christians\" + '\n",
      "  '0.004*\"truth\" + 0.004*\"one\"'),\n",
      " (45,\n",
      "  '0.013*\"edu\" + 0.008*\"god\" + 0.006*\"would\" + 0.006*\"subject\" + 0.005*\"lines\" '\n",
      "  '+ 0.005*\"organization\" + 0.005*\"one\" + 0.005*\"university\" + 0.004*\"hell\" + '\n",
      "  '0.004*\"go\"'),\n",
      " (20,\n",
      "  '0.008*\"one\" + 0.006*\"like\" + 0.006*\"god\" + 0.006*\"would\" + 0.005*\"think\" + '\n",
      "  '0.004*\"way\" + 0.004*\"believe\" + 0.004*\"well\" + 0.004*\"see\" + '\n",
      "  '0.004*\"subject\"'),\n",
      " (62,\n",
      "  '0.007*\"edu\" + 0.006*\"subject\" + 0.006*\"lines\" + 0.006*\"god\" + '\n",
      "  '0.005*\"people\" + 0.005*\"organization\" + 0.005*\"team\" + 0.004*\"see\" + '\n",
      "  '0.004*\"senators\" + 0.004*\"first\"'),\n",
      " (18,\n",
      "  '0.008*\"com\" + 0.007*\"edu\" + 0.006*\"god\" + 0.006*\"lines\" + 0.005*\"maria\" + '\n",
      "  '0.005*\"organization\" + 0.005*\"jesus\" + 0.005*\"would\" + 0.004*\"subject\" + '\n",
      "  '0.004*\"like\"'),\n",
      " (22,\n",
      "  '0.011*\"edu\" + 0.008*\"organization\" + 0.007*\"lines\" + 0.007*\"one\" + '\n",
      "  '0.007*\"subject\" + 0.006*\"god\" + 0.006*\"writes\" + 0.006*\"would\" + '\n",
      "  '0.005*\"article\" + 0.004*\"people\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "\n",
    "#doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict topic of new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‡ You can now use your LDA model to predict the topic of a new text. First, use your vectorizer to vectorize the example. Then, use your LDA model to predict the topic of the vectorized example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From:', 'farenebt@logic.camp.clarkson.edu', '(Droopy)', 'Subject:', 'Re:', 'AHL', 'Calder', 'Cup', 'Playoff', 'preview', 'Organization:', 'Clarkson', 'University', 'Lines:', '37', 'Nntp-Posting-Host:', 'logic.clarkson.edu', 'X-Newsreader:', 'TIN', '[version', '1.1', 'PL8]', 'Daryl', 'Turner', '(umturne4@ccu.umanitoba.ca)', 'wrote:', ':', 'In', 'article', '<1993Apr14.193524.25755@news.clarkson.edu>', 'farenebt@craft.camp.clarkson.edu', '(Droopy)', 'writes:', ':', '>', ':', '>ATLANTIC', 'DIVISION', ':', '>', ':', '>', 'ST', \"JOHN'S\", 'MAPLE', 'LEAFS', 'VS', 'MONCTON', 'HAWKS', ':', '>', 'MONCTON', 'HAWKS', ':', '>See', 'CD', 'Islanders.', 'Moncton', 'is', 'a', 'very', 'similar', 'team', 'to', 'CDI.', 'Low', 'scoring,', ':', '>defensive,', 'good', 'goaltending.', 'John', 'Leblanc', 'and', 'Stu', 'Barnes', 'are', 'the', 'only', ':', '>noticable', 'guns', 'on', 'the', 'team.', 'But', 'the', 'defense', 'is', 'top', 'notch', 'and', ':', '>Mike', \"O'Neill\", 'is', 'the', 'most', 'underrated', 'goalie', 'in', 'the', 'league.', ':', '>', ':', 'Bri,', 'as', 'I', 'have', 'tried', 'to', 'tell', 'you', 'since', '2', 'February,', 'Michael', \"O'Neill\", ':', 'might', 'be', 'the', 'most', 'underrated', 'goalie', 'in', 'the', 'AHL,', 'but', 'he', \"ISN'T\", 'in', 'the', ':', 'AHL.', \"He's\", 'on', 'the', 'Winnipeg', \"Jets'\", 'injury', 'list,', 'as', 'he', 'has', 'been', 'since', ':', 'his', 'first', 'NHL', 'start', 'against', 'the', 'Ottawa', 'Senators.', \"He's\", 'out', 'until', ':', 'next', 'year', 'after', 'surgery', 'to', 'repair', 'a', 'shoulder', 'separation.', ':', 'Stu', 'Barnes', 'might', 'be', 'an', 'AHL', 'gun', 'for', 'the', 'Hawks,', 'but', \"he's\", 'now', 'the', 'third', ':', 'line', 'center', 'with', 'the', 'Jets,', 'and', 'has', 'been', 'since', 'mid', 'January', 'or', 'so.', 'Sorry,', 'my', 'memory', 'is', 'gone.', 'I', 'thought', 'that', \"O'Neill\", 'got', 'sent', 'back', 'down', 'in', 'February', 'but', 'I', 'must', 'have', 'been', 'given', 'incorrect', 'info.', 'I', 'guess', 'this', 'says', 'it', 'all', 'about', 'Moncton', 'because', 'Barnes', 'is', 'still', 'one', 'of', 'their', 'top', '3', 'or', 'so', 'scorers', 'even', 'though', \"he's\", 'been', 'out', 'since', 'January.', '++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++', '+', 'Bri', 'Farenell', 'farenebt@craft.camp.clarkson.edu', '+', '+', 'AHL', 'and', 'ECAC', 'contact', 'for', 'rec.sport.hockey', 'Go', 'USA', 'Hockey!', '+', '+', 'Adirondack', 'Red', 'Wings,', 'Calder', 'Cup', 'Champs:', \"'81\", \"'86\", \"'89\", \"'92\", '+', '+', 'Clarkson', 'Hockey,', 'ECAC', 'Tournament', 'Champs:', \"'66\", \"'91\", \"'93\", '+', '+', 'Glens', 'Falls', 'High', 'Hockey,', 'NY', 'Division', 'II', 'State', 'Champs:', \"'90\", \"'91\", '+', '+', 'AHL', 'fans:', 'join', 'the', 'AHL', 'mailing', 'list:', 'ahl-news-request@andrew.cmu.edu', '+', '+', 'CONGRATS', 'TO', 'THE', 'BOSTON', 'BRUINS,', '1992-93', 'ADAMS', 'DIVISION', 'CHAMPIONS', '+', '+', 'PHOENIX', 'SUNS,', '1992-93', 'PACIFIC', 'DIVISION', 'CHAMPIONS', '+', '++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(57, 0.46415955)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-dccabad636c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#topics = sorted(output,key=lambda x:x[1],reverse=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Programmes2\\Anaconda\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, tokenid)\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;31m# recompute id->word accordingly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrevdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2token\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokenid\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# will throw for non-existent ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (57, 0.46415955)"
     ]
    }
   ],
   "source": [
    "string_input = df['text'].iloc[100]\n",
    "#X = vect.transform(string_input)\n",
    "    # Convert sparse matrix to gensim corpus.\n",
    "\n",
    "print(string_input.split())\n",
    "corpus = id2word.doc2bow(string_input.split())   \n",
    "#corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "output = list(lda_model[corpus])[0]\n",
    "#topics = sorted(output,key=lambda x:x[1],reverse=True)\n",
    "print(id2word[output[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locker\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
